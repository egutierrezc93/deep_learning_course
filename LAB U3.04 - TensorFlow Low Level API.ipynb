{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# LAB: TensorFlow Low level API"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import init\n", "init.init(force_download=False)\n", "init.get_weblink()"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["from local.lib.rlxmoocapi import submit, session\n", "import inspect\n", "student = session.Session(init.endpoint).login( course_id=init.course_id, \n", "                                                session_id=\"UDEA\", \n", "                                                lab_id=\"LAB_U3.04\" )"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["try:\n", "    %tensorflow_version 2.x\n", "    print (\"Using TF2 in Google Colab\")\n", "except:\n", "    pass\n", "import numpy as np\n", "import tensorflow as tf\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "%load_ext tensorboard\n", "\n", "from sklearn.datasets import *\n", "from local.lib import mlutils\n", "tf.__version__"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "**A multilayer perceptron**\n", "\n", "assuming $n$ layers, the output at layer $i$\n", "\n", "$$\\mathbf{a}_i = \\text{activation}(\\mathbf{a}_{i-1} \\cdot \\mathbf{W}_i + \\mathbf{b}_i)$$\n", "\n", "at the first layer\n", "\n", "$$\\mathbf{a}_0 = \\text{activation}(\\mathbf{X} \\cdot \\mathbf{W}_0 + \\mathbf{b}_0)$$\n", "\n", "and the layer prediction is the output of the last layer:\n", "\n", "$$\\hat{\\mathbf{y}} = \\mathbf{a}_{n-1}$$ \n", "\n", "with $\\text{activation}$ being an activation function, such as $\\text{sigmoid}(z) = \\frac{1}{1+e^{-z}}$, $\\text{tanh}$, $\\text{ReLU}$, etc.\n", "\n", "\n", "**Cost (with regularization)**\n", "\n", "\n", "$$J(\\mathbf{b}_1, b_2, \\mathbf{W}_1, \\mathbf{W}_2) = \\frac{1}{m}\\sum_{i=0}^{m-1} (\\hat{y}-y)^2 + \\lambda \\sum_{i=0}^{n-1} \\bigg[ \\| \\mathbf{b}_i\\|^2 + \\|\\mathbf{W}_i\\|^2 \\bigg]$$\n", "\n", "\n", "$\\lambda$ regulates the participation of the regularization terms. Given a vector or matrix $\\mathbf{T}$, its squared norm is denoted by $||\\mathbf{T}||^2 \\in \\mathbb{R}$ and it's computed by squaring all its elements and summing them all up. \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Task 1: Multilayer perceptron model with low level API\n", "\n", "build a custom model (`build`, `call`) using standard optimization (`model.compile`, etc.)\n", "\n", "your class must:\n", "\n", "- accept in the constructor:\n", "   - `neurons`: a list containing the number of neurons of each layer\n", "   - `activations`: a list of strings containing one of `'sigmoid'`, `'tanh'`, `'linear'`, `'relu'` so that the corresponding `tf.keras.activations` function is used.\n", "\n", "- **include Tensorboard callbacks for BOTH LOSS AND ACCURACY. See the [example here](https://www.tensorflow.org/tensorboard/get_started)**. You will have to add the appropriate [keras metric](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) according to your model output.\n", "\n", "- create a **custom loss function** to include the regularization parameter and **use it when compiling the model**. Look in the internet for tutorials, for example [here](https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618), and many others\n", "\n", "for instance, the following code:\n", "\n", "    mlp = MLP(neurons=[10,1], activations=[\"tanh\",\"sigmoid\"])\n", "\n", "must create a MLP with two layers with 10 and 1 neurons each, and `tanh` and `sigmoid` activation correspondingly.\n", "\n", "\n", "Observe that:\n", "\n", "- as you are following the Keras class API (`call`+ `build`) your model should work as any other model (`compile`, `fit`, etc.).\n", "\n", "- the `loss` method of your MLP instance must be passed on when calling `compile`\n", "\n", "- You must use the Model function [add_weight](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_weight) to initialize the weights and bias of your MLP\n", "    - And you must set the `trainable` parameter as `True`\n", "\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["from tensorflow.keras import Model\n", "from progressbar import progressbar as pbar\n", "from tensorflow.keras.activations import relu, sigmoid, tanh, linear"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["def MLP(neurons, activations, reg=0.):\n", "    from tensorflow.keras import Model\n", "    from tensorflow.keras.activations import relu, sigmoid, tanh, linear\n", "    \n", "    class _MLP(Model):\n", "        def __init__(self, neurons, activations, reg=0.):\n", "            super(_MLP, self).__init__()\n", "            assert len(neurons)==len(activations), \"must have the same number of neurons and activations\"\n", "\n", "            ... # YOUR CODE HERE\n", "\n", "        def build(self, input_shape):\n", "            \n", "            self.W = ... # YOUR CODE HERE, must be a list of tensors created with add_weight \n", "            self.b = ... # YOUR CODE HERE, must be a list of tensors created with add_weight\n", "\n", "            ... # YOUR CODE HERE\n", "\n", "        @tf.function\n", "        def call(self, X):\n", "            ... # YOUR CODE HERE\n", "\n", "        @tf.function\n", "        def loss(self, y_true, y_pred):\n", "            ... # YOUR CODE HERE\n", "            \n", "    return _MLP(neurons, activations, reg)"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["X, y = make_moons(200, noise=.35)\n", "X, y = X.astype(np.float32), y.astype(np.float32).reshape(-1,1)\n", "plt.scatter(X[:,0][y[:,0]==0], X[:,1][y[:,0]==0], color=\"red\", label=\"class 0\")\n", "plt.scatter(X[:,0][y[:,0]==1], X[:,1][y[:,0]==1], color=\"blue\", label=\"class 1\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!rm -rf logs\n", "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/no_regularization\")\n", "\n", "mlp = MLP(neurons=[10,1], activations=[\"tanh\",\"sigmoid\"])\n", "\n", "mlp.compile(... # YOUR CODE HERE)"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["mlp.fit(X,y, epochs=400, batch_size=16, verbose=0, \n", "        callbacks=[tensorboard_callback])"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["predict = lambda X: (mlp.predict(X)[:,0]>0.5).astype(int)\n", "mlutils.plot_2Ddata_with_boundary(predict, X, y.reshape(-1));\n", "plt.title(\"accuracy %.2f\"%np.mean(predict(X)==y.reshape(-1)));"]}, {"cell_type": "markdown", "metadata": {}, "source": ["regularization must work!!!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/with_regularization\")\n", "mlp = MLP(neurons=[10,1], activations=[\"tanh\",\"sigmoid\"], reg=0.001)\n", "\n", "mlp.compile(... # YOUR CODE HERE)\n", "\n", "mlp.fit(X,y, epochs=400, batch_size=10, verbose=0, callbacks=[tensorboard_callback])\n", "mlutils.plot_2Ddata_with_boundary(predict, X, y.reshape(-1))\n", "plt.title(\"accuracy %.2f\"%np.mean(predict(X)==y.reshape(-1)));"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["%load_ext tensorboard\n", "%tensorboard --logdir logs"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Submit your solution**"]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["student.submit_task(namespace=globals(), course_id=init.course_id, lab_id='LAB_U3.04', task_id='task_01')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Task 2: Multilayer perceptron model AND optimization loop with low level API\n", "\n", "build a custom model such as in the exercise above and implement your optimization loop in `.fit`\n", " \n", "observe that you will have to:\n", "- use whichever method you choose from the corresponding notebook (custom SGD, `apply_gradients`, `train_on_batch`)\n", "- write by hand loss and accuracy to Tensorboard. See  how[`tf.summary`](https://www.tensorflow.org/api_docs/python/tf/summary) works\n"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": ["def MLP(neurons, activations, reg=0.):\n", "    from tensorflow.keras import Model\n", "    from progressbar import progressbar as pbar\n", "    from tensorflow.keras.activations import relu, sigmoid, tanh, linear\n", "    \n", "    class _MLP(Model):\n", "        def __init__(self, neurons, activations, reg=0.):\n", "            super(_MLP, self).__init__()\n", "            assert len(neurons)==len(activations), \"must have the same number of neurons and activations\"\n", "\n", "            ... # YOUR CODE HERE\n", "\n", "        def build(self, input_shape):\n", "            \n", "            self.W = ... # YOUR CODE HERE, must be a list of tensors created with add_weight \n", "            self.b = ... # YOUR CODE HERE, must be a list of tensors created with add_weight\n", "\n", "            ... # YOUR CODE HERE\n", "\n", "\n", "        @tf.function\n", "        def call(self, X):\n", "            ... # YOUR CODE HERE\n", "\n", "\n", "        @tf.function\n", "        def loss(self, y_true, y_pred):\n", "            ... # YOUR CODE HERE\n", "            \n", "\n", "        def fit(self, X, y, epochs, batch_size):\n", "            ... # YOUR CODE HERE\n", "\n", "    return _MLP(neurons, activations, reg)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "... use tensorboard!!! ...\n", "\n", "mlp = MLP(neurons=[10,1], activations=[\"tanh\",\"sigmoid\"])\n", "mlp.compile(... # YOUR CODE HERE)\n", "\n", "mlp.fit(X,y, epochs=400, batch_size=16)"]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": ["mlutils.plot_2Ddata_with_boundary(predict, X, y.reshape(-1));\n", "plt.title(\"accuracy %.2f\"%np.mean(predict(X)==y.reshape(-1)));"]}, {"cell_type": "markdown", "metadata": {}, "source": ["regularization must work!!!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["... use tensorboard!!! ...\n", "\n", "mlp = MLP(neurons=[10,1], activations=[\"tanh\",\"sigmoid\"], reg=0.01)\n", "mlp.compile(... # YOUR CODE HERE)\n", "\n", "mlp.fit(X,y, epochs=400, batch_size=16)"]}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [], "source": ["mlutils.plot_2Ddata_with_boundary(predict, X, y.reshape(-1));\n", "plt.title(\"accuracy %.2f\"%np.mean(predict(X)==y.reshape(-1)));"]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [], "source": ["student.submit_task(namespace=globals(), course_id=init.course_id, lab_id='LAB_U3.04', task_id='task_02')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.10"}}, "nbformat": 4, "nbformat_minor": 2}